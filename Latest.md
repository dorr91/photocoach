# PhotoCoach - Architecture Overview

## What We Built

An AI-powered photo coaching app for iOS. Take a photo â†’ get instant feedback on composition, lighting, and subject from GPT-4 Vision.

## Tech Stack

- **SwiftUI** with MVVM pattern
- **AVFoundation** for camera (bridged to SwiftUI)
- **Core Data** for persistence
- **Keychain** for secure API key storage
- **URLSession** with async/await for streaming API responses

## App Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     tap shutter     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CameraView  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  PhotoReviewView â”‚
â”‚             â”‚                     â”‚                  â”‚
â”‚  [shutter]  â”‚ â—„â”€â”€â”€â”€â”€â”€ back â”€â”€â”€â”€â”€â”€ â”‚  [photo cards]   â”‚
â”‚  [settings] â”‚                     â”‚  [AI feedback]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ gear icon
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SettingsViewâ”‚
â”‚             â”‚
â”‚ [API key]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

### Framework-Based Architecture

PhotoCoach now uses a **Swift Package** (PhotoCoachCore) to extract business logic, enabling fast unit tests without simulator overhead:

```
PhotoCoach/
â”œâ”€â”€ PhotoCoachCore/              # ğŸ†• Swift Package (business logic)
â”‚   â”œâ”€â”€ Package.swift           # Package manifest
â”‚   â”œâ”€â”€ Sources/PhotoCoachCore/
â”‚   â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â”‚   â”œâ”€â”€ CoreDataEntities.swift
â”‚   â”‚   â”‚   â””â”€â”€ PhotoCoach.xcdatamodeld     # Core Data model
â”‚   â”‚   â”œâ”€â”€ Protocols/           # Protocol abstractions
â”‚   â”‚   â”‚   â”œâ”€â”€ CoreDataStackProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ KeychainServiceProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ PhotoStorageProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAIServiceProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ URLSessionProtocol.swift
â”‚   â”‚   â”‚   â””â”€â”€ FileManagerProtocol.swift
â”‚   â”‚   â”œâ”€â”€ Services/            # Testable business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ KeychainService.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ PhotoStorageService.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ SimpleCoreDataStack.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ MockOpenAIService.swift
â”‚   â”‚   â”‚   â””â”€â”€ ServiceContainer.swift     # DI container
â”‚   â”‚   â””â”€â”€ ViewModels/          # (Temporarily disabled)
â”‚   â””â”€â”€ Tests/PhotoCoachCoreTests/
â”‚       â””â”€â”€ BasicPackageTests.swift       # Fast unit tests (~0.07s)
â”‚
â”œâ”€â”€ PhotoCoach/                  # ğŸ”„ iOS App (UI + integration)
â”‚   â”œâ”€â”€ PhotoCoachApp.swift      # App entry point, uses ServiceContainer
â”‚   â”œâ”€â”€ ContentView.swift        # Navigation container
â”‚   â”œâ”€â”€ Camera/                  # Camera feature
â”‚   â”‚   â”œâ”€â”€ CameraManager.swift
â”‚   â”‚   â”œâ”€â”€ CameraPreview.swift
â”‚   â”‚   â””â”€â”€ CameraView.swift
â”‚   â”œâ”€â”€ Views/                   # SwiftUI screens
â”‚   â”‚   â”œâ”€â”€ PhotoReviewView.swift
â”‚   â”‚   â”œâ”€â”€ PhotoCard.swift
â”‚   â”‚   â””â”€â”€ SettingsView.swift
â”‚   â”œâ”€â”€ ViewModels/
â”‚   â”‚   â””â”€â”€ FeedbackViewModel.swift
â”‚   â””â”€â”€ Services/                # Legacy services (being migrated)
â”‚       â”œâ”€â”€ CoreDataStack.swift
â”‚       â”œâ”€â”€ OpenAIService.swift
â”‚       â””â”€â”€ ServiceContainer.swift
â”‚
â””â”€â”€ PhotoCoachTests/             # Integration tests (~30s)
    â”œâ”€â”€ Tests/
    â”œâ”€â”€ Mocks/
    â””â”€â”€ Helpers/
```

### Key Architectural Benefits

1. **Fast Testing**: Business logic tests run in 0.07s vs 2-5+ minutes
2. **Platform Abstraction**: Uses `PlatformImage` typealias for UIKit/AppKit compatibility  
3. **Dependency Injection**: `ServiceContainer` provides clean testing boundaries
4. **Protocol-Based Design**: All services implement testable protocols
5. **Modular Code**: Business logic separate from UI concerns

### Testing Strategy

#### Fast Unit Tests (PhotoCoachCore Package)
```bash
cd PhotoCoachCore && swift test
```
- **Execution Time**: ~0.07 seconds (100x faster than before)
- **No Simulator**: Pure Swift package tests
- **Real Logic**: Tests actual KeychainService, PhotoStorageService, etc.
- **Coverage**: Protocol conformance, business logic, error handling

#### Integration Tests (Xcode Target)
```bash
xcodebuild test -scheme PhotoCoach
```
- **Execution Time**: ~30 seconds (with simulator)
- **Full Stack**: Tests complete app integration
- **UI Testing**: Critical user flows only

This dual approach enables rapid TDD cycles while maintaining comprehensive coverage.

## Key Components

### Camera (UIKit Bridge)

The camera requires AVFoundation (UIKit), so we bridge it to SwiftUI:

| File | Purpose |
|------|---------|
| `CameraManager` | `@MainActor ObservableObject` that manages AVCaptureSession, handles permissions, captures photos. Tracks device orientation via `UIDevice` notifications and sets `videoRotationAngle` on capture for correct landscape/portrait photos. |
| `CameraPreview` | `UIViewRepresentable` that wraps AVCaptureVideoPreviewLayer for the live preview |
| `CameraView` | SwiftUI view with shutter button, last photo thumbnail, settings gear. UI locked to portrait, but photos capture in correct orientation based on device rotation. |

### Data Flow

```
Photo captured
     â”‚
     â–¼
PhotoStorage.savePhoto()     â†’ Saves JPEG to Documents/Photos/
     â”‚                       â†’ Generates thumbnail
     â–¼
CoreDataStack.createPhoto()  â†’ Creates Photo entity with paths
     â”‚
     â–¼
CoreDataStack.createFeedback() â†’ Creates empty AIFeedback entity
     â”‚
     â–¼
Navigate to PhotoReviewView
     â”‚
     â–¼
FeedbackViewModel.fetchFeedback()
     â”‚
     â–¼
OpenAIService.streamFeedback() â†’ Sends image to GPT-4 Vision
     â”‚                         â†’ Streams response chunks
     â–¼
FeedbackViewModel updates state â†’ PhotoCard displays streaming text
     â”‚
     â–¼
CoreDataStack.updateFeedback() â†’ Saves complete feedback
```

### Storage

| What | Where | How |
|------|-------|-----|
| Photo files | `Documents/Photos/*.jpg` | FileManager via PhotoStorage |
| Thumbnails | `Documents/Thumbnails/*_thumb.jpg` | FileManager via PhotoStorage |
| Metadata | Core Data | Photo & AIFeedback entities |
| API Key | Keychain | KeychainHelper (Security framework) |

### AI Integration

`OpenAIService` (singleton) uses the OpenAI Responses API with session continuity:

1. Image resized to max 1024px and converted to base64
2. POST to `/v1/responses` with `stream: true` and `store: true`
3. On subsequent photos, `previous_response_id` chains to prior response (OpenAI remembers context server-side)
4. Parse SSE chunks, yield text via `AsyncThrowingStream`
5. `FeedbackViewModel` accumulates chunks and updates `@Published` state
6. SwiftUI automatically re-renders as text streams in

This allows the AI coach to reference patterns across multiple photos in a session without re-sending previous images.

### Error Handling

- **No API key** â†’ Error state with "go to settings" prompt
- **Network failure** â†’ Retry button on PhotoCard
- **Camera denied** â†’ Permission denied view with "Open Settings" button

## Previews

All views have `#Preview` blocks for Xcode Canvas:
- `SettingsView` - Works fully
- `PhotoReviewView` - Shows empty state
- `PhotoCard` - Shows with mock photo
- `CameraView` - Shows permission denied state (no camera in simulator)
