# PhotoCoach - Architecture Overview

**Last updated**: Commit `04d38e9` (2026-01-02)

## What We Built

An AI-powered photo coaching app for iOS. Take a photo â†’ get instant feedback on composition, lighting, and subject from GPT-4 Vision.

## Recent Changes

### Single-Photo Detail View with Persistent Followups (04d38e9)
- Added `PhotoDetailView` for viewing individual photos with their AI feedback
- Followup conversations are now persistent â€” continue coaching sessions across app launches
- Photos can be tapped from the review grid to open detailed view

### Photo Library Picker (aca9c5a)
- Added ability to import photos from the photo library (not just camera)
- Camera view now shows a photo library button alongside the shutter

### Grid Overlay (2e6505f)
- Added a rule-of-thirds grid overlay to the camera view for composition guidance

### Performance & Stability Fixes
- **UI freezing fixed** (0c9fc49): Debounced streaming feedback updates to prevent UI lockups
- **Main thread optimization** (b639e3b, 6938e09, ca81475): Moved image loading, resizing, and photo fetching off main thread
- **Infinite recursion bug fixed** (d1ea59c): Fixed app freeze caused by recursive calls
- **Swift 6 Sendable warnings** (1562e92): Fixed concurrency warnings in PhotoReviewView

### AI Coaching Improvements
- **Session continuity** (7b97a2c): Single OpenAI session so critiques can build on each other
- **Improved prompts** (4462ed2, e353e1a): Better coaching prompts with reshoot suggestions
- **Increased token limit** (ddaf7c1): Higher LLM token limit for more detailed feedback
- **Portrait mode handling** (0ae4bbc): Better handling of portrait orientation photos

### Architecture
- **Testable package** (ca4109a): Moved business logic into PhotoCoachCore Swift package for fast unit tests

## Tech Stack

- **SwiftUI** with MVVM pattern
- **AVFoundation** for camera (bridged to SwiftUI)
- **Core Data** for persistence
- **Keychain** for secure API key storage
- **URLSession** with async/await for streaming API responses

## App Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     tap shutter     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    tap photo    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CameraView  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  PhotoReviewView â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ PhotoDetailView â”‚
â”‚             â”‚                     â”‚                  â”‚                 â”‚                 â”‚
â”‚  [shutter]  â”‚ â—„â”€â”€â”€â”€â”€â”€ back â”€â”€â”€â”€â”€â”€ â”‚  [photo grid]    â”‚ â—„â”€â”€â”€â”€ back â”€â”€â”€â”€ â”‚ [full feedback] â”‚
â”‚  [library]  â”‚                     â”‚  [AI feedback]   â”‚                 â”‚ [followups]     â”‚
â”‚  [settings] â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ gear icon
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SettingsViewâ”‚
â”‚             â”‚
â”‚ [API key]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

### Framework-Based Architecture

PhotoCoach now uses a **Swift Package** (PhotoCoachCore) to extract business logic, enabling fast unit tests without simulator overhead:

```
PhotoCoach/
â”œâ”€â”€ PhotoCoachCore/              # ğŸ†• Swift Package (business logic)
â”‚   â”œâ”€â”€ Package.swift           # Package manifest
â”‚   â”œâ”€â”€ Sources/PhotoCoachCore/
â”‚   â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â”‚   â”œâ”€â”€ CoreDataEntities.swift
â”‚   â”‚   â”‚   â””â”€â”€ PhotoCoach.xcdatamodeld     # Core Data model
â”‚   â”‚   â”œâ”€â”€ Protocols/           # Protocol abstractions
â”‚   â”‚   â”‚   â”œâ”€â”€ CoreDataStackProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ KeychainServiceProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ PhotoStorageProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAIServiceProtocol.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ URLSessionProtocol.swift
â”‚   â”‚   â”‚   â””â”€â”€ FileManagerProtocol.swift
â”‚   â”‚   â”œâ”€â”€ Services/            # Testable business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ KeychainService.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ PhotoStorageService.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ SimpleCoreDataStack.swift
â”‚   â”‚   â”‚   â”œâ”€â”€ MockOpenAIService.swift
â”‚   â”‚   â”‚   â””â”€â”€ ServiceContainer.swift     # DI container
â”‚   â”‚   â””â”€â”€ ViewModels/          # (Temporarily disabled)
â”‚   â””â”€â”€ Tests/PhotoCoachCoreTests/
â”‚       â””â”€â”€ BasicPackageTests.swift       # Fast unit tests (~0.07s)
â”‚
â”œâ”€â”€ PhotoCoach/                  # ğŸ”„ iOS App (UI + integration)
â”‚   â”œâ”€â”€ PhotoCoachApp.swift      # App entry point, uses ServiceContainer
â”‚   â”œâ”€â”€ ContentView.swift        # Navigation container
â”‚   â”œâ”€â”€ Camera/                  # Camera feature
â”‚   â”‚   â”œâ”€â”€ CameraManager.swift
â”‚   â”‚   â”œâ”€â”€ CameraPreview.swift
â”‚   â”‚   â”œâ”€â”€ CameraView.swift
â”‚   â”‚   â””â”€â”€ GridOverlay.swift    # Rule-of-thirds overlay
â”‚   â”œâ”€â”€ Views/                   # SwiftUI screens
â”‚   â”‚   â”œâ”€â”€ PhotoReviewView.swift
â”‚   â”‚   â”œâ”€â”€ PhotoDetailView.swift  # Single photo with persistent followups
â”‚   â”‚   â”œâ”€â”€ PhotoCard.swift
â”‚   â”‚   â””â”€â”€ SettingsView.swift
â”‚   â”œâ”€â”€ ViewModels/
â”‚   â”‚   â””â”€â”€ FeedbackViewModel.swift
â”‚   â”œâ”€â”€ Protocols/               # Protocol abstractions (duplicated for app target)
â”‚   â””â”€â”€ Services/                # App services
â”‚       â”œâ”€â”€ CoreDataStack.swift
â”‚       â”œâ”€â”€ OpenAIService.swift
â”‚       â”œâ”€â”€ OpenAIServiceExtensions.swift
â”‚       â”œâ”€â”€ KeychainService.swift
â”‚       â”œâ”€â”€ PhotoStorageService.swift
â”‚       â””â”€â”€ ServiceContainer.swift
â”‚
â””â”€â”€ PhotoCoachTests/             # Integration tests (~30s)
    â”œâ”€â”€ Tests/
    â”œâ”€â”€ Mocks/
    â””â”€â”€ Helpers/
```

### Key Architectural Benefits

1. **Fast Testing**: Business logic tests run in 0.07s vs 2-5+ minutes
2. **Platform Abstraction**: Uses `PlatformImage` typealias for UIKit/AppKit compatibility  
3. **Dependency Injection**: `ServiceContainer` provides clean testing boundaries
4. **Protocol-Based Design**: All services implement testable protocols
5. **Modular Code**: Business logic separate from UI concerns

### Testing Strategy

#### Fast Unit Tests (PhotoCoachCore Package)
```bash
cd PhotoCoachCore && swift test
```
- **Execution Time**: ~0.07 seconds (100x faster than before)
- **No Simulator**: Pure Swift package tests
- **Real Logic**: Tests actual KeychainService, PhotoStorageService, etc.
- **Coverage**: Protocol conformance, business logic, error handling

#### Integration Tests (Xcode Target)
```bash
xcodebuild test -scheme PhotoCoach
```
- **Execution Time**: ~30 seconds (with simulator)
- **Full Stack**: Tests complete app integration
- **UI Testing**: Critical user flows only

This dual approach enables rapid TDD cycles while maintaining comprehensive coverage.

## Key Components

### Camera (UIKit Bridge)

The camera requires AVFoundation (UIKit), so we bridge it to SwiftUI:

| File | Purpose |
|------|---------|
| `CameraManager` | `@MainActor ObservableObject` that manages AVCaptureSession, handles permissions, captures photos. Tracks device orientation via `UIDevice` notifications and sets `videoRotationAngle` on capture for correct landscape/portrait photos. |
| `CameraPreview` | `UIViewRepresentable` that wraps AVCaptureVideoPreviewLayer for the live preview |
| `CameraView` | SwiftUI view with shutter button, last photo thumbnail, settings gear. UI locked to portrait, but photos capture in correct orientation based on device rotation. |

### Data Flow

```
Photo captured
     â”‚
     â–¼
PhotoStorage.savePhoto()     â†’ Saves JPEG to Documents/Photos/
     â”‚                       â†’ Generates thumbnail
     â–¼
CoreDataStack.createPhoto()  â†’ Creates Photo entity with paths
     â”‚
     â–¼
CoreDataStack.createFeedback() â†’ Creates empty AIFeedback entity
     â”‚
     â–¼
Navigate to PhotoReviewView
     â”‚
     â–¼
FeedbackViewModel.fetchFeedback()
     â”‚
     â–¼
OpenAIService.streamFeedback() â†’ Sends image to GPT-4 Vision
     â”‚                         â†’ Streams response chunks
     â–¼
FeedbackViewModel updates state â†’ PhotoCard displays streaming text
     â”‚
     â–¼
CoreDataStack.updateFeedback() â†’ Saves complete feedback
```

### Storage

| What | Where | How |
|------|-------|-----|
| Photo files | `Documents/Photos/*.jpg` | FileManager via PhotoStorage |
| Thumbnails | `Documents/Thumbnails/*_thumb.jpg` | FileManager via PhotoStorage |
| Metadata | Core Data | Photo & AIFeedback entities |
| API Key | Keychain | KeychainHelper (Security framework) |

### AI Integration

`OpenAIService` (singleton) uses the OpenAI Responses API with session continuity:

1. Image resized to max 1024px and converted to base64
2. POST to `/v1/responses` with `stream: true` and `store: true`
3. On subsequent photos, `previous_response_id` chains to prior response (OpenAI remembers context server-side)
4. Parse SSE chunks, yield text via `AsyncThrowingStream`
5. `FeedbackViewModel` accumulates chunks and updates `@Published` state
6. SwiftUI automatically re-renders as text streams in

This allows the AI coach to reference patterns across multiple photos in a session without re-sending previous images.

### Error Handling

- **No API key** â†’ Error state with "go to settings" prompt
- **Network failure** â†’ Retry button on PhotoCard
- **Camera denied** â†’ Permission denied view with "Open Settings" button

## Previews

All views have `#Preview` blocks for Xcode Canvas:
- `SettingsView` - Works fully
- `PhotoReviewView` - Shows empty state
- `PhotoDetailView` - Shows photo with feedback and followup input
- `PhotoCard` - Shows with mock photo
- `CameraView` - Shows permission denied state (no camera in simulator)
